{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Semantic_Streamline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOrcxJoH8I6mu7JlwLrrUiA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jaynil1611/LAB_2020/blob/master/Semantic_Streamline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AF94iNYZQmlg",
        "colab_type": "code",
        "outputId": "f09d2ac2-64f8-4964-e5e9-a0c9f15a60a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 756
        }
      },
      "source": [
        "!pip install sentence-transformers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentence-transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/32/e3d405806ea525fd74c2c79164c3f7bc0b0b9811f27990484c6d6874c76f/sentence-transformers-0.2.5.1.tar.gz (52kB)\n",
            "\r\u001b[K     |██████▎                         | 10kB 15.9MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 30kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 2.1MB/s \n",
            "\u001b[?25hCollecting transformers==2.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/10/aeefced99c8a59d828a92cc11d213e2743212d3641c87c82d61b035a7d5c/transformers-2.3.0-py3-none-any.whl (447kB)\n",
            "\r\u001b[K     |▊                               | 10kB 20.9MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20kB 27.7MB/s eta 0:00:01\r\u001b[K     |██▏                             | 30kB 33.3MB/s eta 0:00:01\r\u001b[K     |███                             | 40kB 37.6MB/s eta 0:00:01\r\u001b[K     |███▋                            | 51kB 4.4MB/s eta 0:00:01\r\u001b[K     |████▍                           | 61kB 5.1MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 71kB 5.9MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 81kB 6.6MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 92kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 102kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████                        | 112kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 122kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 133kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 143kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████                     | 153kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 163kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 174kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 184kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 194kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 204kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 215kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 225kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 235kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 245kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 256kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 266kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 276kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 286kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 296kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 307kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 317kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 327kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 337kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 348kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 358kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 368kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 378kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 389kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 399kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 409kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 419kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 430kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 440kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 450kB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (4.38.0)\n",
            "Requirement already satisfied: torch>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.18.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 50.5MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 42.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0->sentence-transformers) (1.12.23)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0->sentence-transformers) (2.21.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sentence-transformers) (0.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->sentence-transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.3.0->sentence-transformers) (7.1.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0->sentence-transformers) (0.9.5)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.23 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0->sentence-transformers) (1.15.23)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0->sentence-transformers) (0.3.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0->sentence-transformers) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0->sentence-transformers) (2019.11.28)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.23->boto3->transformers==2.3.0->sentence-transformers) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.23->boto3->transformers==2.3.0->sentence-transformers) (0.15.2)\n",
            "Building wheels for collected packages: sentence-transformers, sacremoses\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-0.2.5.1-cp36-none-any.whl size=67076 sha256=091aadb41399bdfedf9f8138a601f647add66289579e5bb0ed8b0f80d2c07d9b\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/ca/b4/7ca542b411730a8840f8e090df2ddacffa1c4dd9f209684c19\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=f679c5f466b2007a1f131e7beb243535edb7a52eac63d9ab88cc7c3b675c7dcc\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sentence-transformers sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, transformers, sentence-transformers\n",
            "Successfully installed sacremoses-0.0.38 sentence-transformers-0.2.5.1 sentencepiece-0.1.85 transformers-2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjcCcyrrQp2c",
        "colab_type": "code",
        "outputId": "3af87163-b636-40bf-cf98-212cf5279a67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgjUUFgXRoTP",
        "colab_type": "code",
        "outputId": "f269c6df-e596-492e-971c-c848617a125b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "import pandas as pd\n",
        "# df = pd.read_csv(\"news_17_18_19_20.csv\")\n",
        "df = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/news_17_18_19_20_21_22_23_24.csv\")\n",
        "df.drop(\"Unnamed: 0\",axis=1,inplace=True)\n",
        "df.head()\n",
        "df.shape"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3423, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-Y0n7FsQ0ZV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import scipy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4N-UBOAQ9ne",
        "colab_type": "code",
        "outputId": "2eeb9a9d-147d-4c4b-90d3-f943b16ef393",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        }
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load the BERT model. Various models trained on Natural Language Inference (NLI) https://github.com/UKPLab/sentence-transformers/blob/master/docs/pretrained-models/nli-models.md and \n",
        "# Semantic Textual Similarity are available https://github.com/UKPLab/sentence-transformers/blob/master/docs/pretrained-models/sts-models.md\n",
        "\n",
        "model = SentenceTransformer('bert-large-nli-stsb-mean-tokens')\n",
        "# model = SentenceTransformer('roberta-large-nli-stsb-mean-tokens')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1.24G/1.24G [00:46<00:00, 26.6MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVF0Ofqk2Hjq",
        "colab_type": "code",
        "outputId": "ee5ba96d-0430-422f-8e26-1fae2b3b1b80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SentenceTransformer(\n",
              "  (0): BERT(\n",
              "    (bert): BertModel(\n",
              "      (embeddings): BertEmbeddings(\n",
              "        (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
              "        (position_embeddings): Embedding(512, 1024)\n",
              "        (token_type_embeddings): Embedding(2, 1024)\n",
              "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (encoder): BertEncoder(\n",
              "        (layer): ModuleList(\n",
              "          (0): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (1): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (2): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (3): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (4): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (5): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (6): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (7): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (8): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (9): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (10): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (11): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (12): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (13): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (14): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (15): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (16): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (17): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (18): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (19): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (20): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (21): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (22): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (23): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (pooler): BertPooler(\n",
              "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (activation): Tanh()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (1): Pooling()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfSjmDTIWS--",
        "colab_type": "code",
        "outputId": "419b3954-ae32-41dc-81df-072f653d9cf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "%cd /content/drive/My Drive/Colab Notebooks/Probabilistic-RNN-DA-Classifier\n",
        "import datetime\n",
        "import time\n",
        "from keras.models import load_model\n",
        "from utilities import *"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/Probabilistic-RNN-DA-Classifier\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xbf1y5J6npHp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "resource_dir = 'data/'\n",
        "embeddings_dir = \"embeddings/\"\n",
        "model_dir = 'models/'\n",
        "model_name = 'Probabilistic Model'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsGfOXorndvl",
        "colab_type": "code",
        "outputId": "21eb365e-1b10-4ab1-efc1-312f5b4b93aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "source": [
        "# Load metadata\n",
        "metadata = load_data(resource_dir + \"metadata.pkl\")\n",
        "word_frequency = 2\n",
        "frequency_data = load_data(embeddings_dir + 'probabilistic_freq_' + str(word_frequency) + '.pkl')\n",
        "\n",
        "# Load Training and test sets\n",
        "train_data = load_data(resource_dir + 'train_data.pkl')\n",
        "train_x, train_y = generate_probabilistic_embeddings(train_data, frequency_data, metadata)\n",
        "\n",
        "test_data = load_data(resource_dir + 'test_data.pkl')\n",
        "test_x, test_y = generate_probabilistic_embeddings(test_data, frequency_data, metadata)\n",
        "\n",
        "val_data = load_data(resource_dir + 'val_data.pkl')\n",
        "val_x, val_y = generate_probabilistic_embeddings(val_data, frequency_data, metadata)\n",
        "\n",
        "# Parameters\n",
        "vocabulary_size = metadata['vocabulary_size']\n",
        "num_labels = metadata['num_labels']\n",
        "max_utterance_len = metadata['max_utterance_len']\n",
        "batch_size = 100\n",
        "hidden_layer = 128\n",
        "learning_rate = 0.001\n",
        "num_epoch = 10\n",
        "model_name = model_name + \" -\" + \\\n",
        "             \" Epochs=\" + str(num_epoch) + \\\n",
        "             \" Hidden Layers=\" + str(hidden_layer)\n",
        "\n",
        "print(\"------------------------------------\")\n",
        "print(\"Using parameters...\")\n",
        "print(\"Vocabulary size: \", vocabulary_size)\n",
        "print(\"Number of labels: \", num_labels)\n",
        "print(\"Word frequency: \", word_frequency)\n",
        "print(\"Batch size: \", batch_size)\n",
        "print(\"Hidden layer size: \", hidden_layer)\n",
        "print(\"learning rate: \", learning_rate)\n",
        "print(\"Epochs: \", num_epoch)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded data from file data/metadata.pkl.\n",
            "Loaded data from file embeddings/probabilistic_freq_2.pkl.\n",
            "Loaded data from file data/train_data.pkl.\n",
            "Loaded data from file data/test_data.pkl.\n",
            "Loaded data from file data/val_data.pkl.\n",
            "------------------------------------\n",
            "Using parameters...\n",
            "Vocabulary size:  23103\n",
            "Number of labels:  41\n",
            "Word frequency:  2\n",
            "Batch size:  100\n",
            "Hidden layer size:  128\n",
            "learning rate:  0.001\n",
            "Epochs:  10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKyds98xgew6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def listToString(s):  \n",
        "    \n",
        "    # initialize an empty string \n",
        "    str1 = \"\"  \n",
        "    \n",
        "    # traverse in the string   \n",
        "    for ele in s:  \n",
        "        str1 += ele +\"\\n\"   \n",
        "    \n",
        "    # return string   \n",
        "    return str1  \n",
        "\n",
        "def gen_test(string):\n",
        "    lines=string.split('\\n');\n",
        "    res={'utterances':[],'labels':[]}\n",
        "    for line in lines:\n",
        "        res['utterances'].append(line.split(' '))\n",
        "        res['labels'].append('%')\n",
        "    return  generate_probabilistic_embeddings(res, frequency_data, metadata)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhyngSKSfcBJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_dir = 'models/'\n",
        "model_name = 'Probabilistic Model'\n",
        "\n",
        "model_claim = load_model('/content/drive/My Drive/Colab Notebooks/Probabilistic-RNN-DA-Classifier/models/Probabilistic Model - Epochs=10 Hidden Layers=128.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qk1TbFf3gQtM",
        "colab_type": "code",
        "outputId": "3c4ab47b-aaf3-4b5c-93ed-cdf4832b53b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/o.csv')\n",
        "d = list(df.Decrypted_Raw_Message.str.strip())\n",
        "s = listToString(d)\n",
        "s"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Toh guys kya karna he\\n@917506854640 tu kal nai he toh aaj research kar konse topic pe karna he hume\\nEspecially GAN\\nWhy this new group?\\nKeep one only\\nSudhanshu aur wo be wala he na\\nToh isme\\nBaat kar sakte he\\nHalbe Ka mail id kya hai?\\nGive respect\\nFirst share with us for problem statement\\nThis message was deleted\\naparna_halbe@spit.ac\\nAre dataset bataya Nahi hai usko\\nHalbe mam bil\\nOk sir\\nSearch for research statement\\nBhai kal karna he ki nai?\\nGenomics â€“ Genomics is the study of DNA of organisms. Machine Learning systems can help in finding the location of protein-encoding genes in a DNA structure. Gene prediction is performed by using two types of searches named as extrinsic and intrinsic. Machine Learning is used in problems related to DNA alignment.\\nSearch for thee\\nTu ispe search karke ye group pe links daal @917303477011\\n@917506854640 tu tuje jo topic pasand he uspe kar and send\\nMe mera marta hu fir topic decide krte he\\n<Media omitted>\\nGuys aaj final karo\\nhttps://www.linkedin.com/posts/digitalis-kapha-b-v-_caeli-climatechange-ai-activity-6625707297471508480-35cx\\n#MP2\\nYou'll decide , I am out today\\nBhai aaj karna chalu karo toh kal mail kar denge\\nRply once read\\nAck\\nSearch for papers\\nSearch for News-Generator model already existing\\nOkayy\\nYep\\nSend papers\\nYou'll have sear\\nsearched\\nHaan\\nIn sometime\\nTuje mile wo send kar\\nAur kal tak likhna he bhai toh hi check hoga monday ko\\nHa\\nAre suno\\nWed abl me sir ne bola tha\\nJis conference me paper daalo ge udhar jo current topic chal raha he uspe karo\\nHave mailed\\nhttps://10times.com/india/technology/conferences\\nhttps://www.pantechsolutions.net/blog/artificial-intelligence-ai-projects/\\nhttps://towardsdatascience.com/generating-new-ideas-for-machine-learning-projects-through-machine-learning-ce3fee50ec2\\nThis is awesome\\nWhat will we do\\nWe will stick to our idea\\n.\\nI meant ye code diya he\\nNo it is transfer learning in NLP\\nO\\nYes , I am talking about implementation\\nAlternative to GAN\\nToh will we stick to news article only?\\nYes\\n@917506854640 rply\\nThis guy has got logic in th text generated that's exactly what we want\\nHaa par\\nAre bhai aaj karo ?\\nYes\\nhttps://towardsdatascience.com/how-i-used-natural-language-processing-to-extract-context-from-news-headlines-df2cf5181ca6\\nhttps://medium.com/@ageitgey/deepfaking-the-news-with-nlp-and-transformer-models-5e057ebd697d\\nhttps://blog.parse.ly/post/7790/machine-learning-nlp-parse-ly-currents/\\nKal 9 am sharp colg pahoch jana dono 9-11 me khatam kar denge\\n<Media omitted>\\nðŸ˜‚ðŸ˜‚ðŸ‘ŒðŸ»ðŸ¤¦ðŸ»â€â™‚\\nSubmit kal karenge\\nK\\nSearch for reference papers\\nAre humlog upload kar dete he\\nAyushi ka group kar raha he\\nK\\nTry to search about\\r\\n What's farzo\\nWhat's farzi \\r\\n Detection techniques\\nHaa\\n11% kisme aya @917303477011\\nNo\\nNp\\nI have done it\\nHaaðŸ‘ðŸ»\\nSee the new resuly\\nresults\\nYess barabar he\\nIt is similar to Turnitin\\n& effective tool\\nHaa acha he\\nhttps://www.duplichecker.com/\\nNow I am sending to Aparna mam\\nHaa\\nKab karna he feasibility studyCv\\n?*\\n@917506854640 @917303477011\\nTuesday\\nAcha\\nGuys try to do scraping of web.whats app .com\\nYash we will do Feasibility study tomorrow evening\\nOkayy\\nAaj karna he ki kal subah?\\nAaj\\nOk\\n6 pm\\nOk\\nTell Jaswant about our project\\nKya bolu\\nSend abstract document\\nTo him\\nAre usko already bola\\nNene\\nMene\\nK\\nI have shared the document\\nComing\\nhttps://www.kaggle.com/c/fake-news/overview\\n@918169471348\\nYe us k news ka he\\nFir whatsapp ka dataset bhi US ka chahiye\\nIt is for only training purpose only\\nAfter training we will apply what's app data\\nScan all the notebooks and analyse the common  algorithm\\nWhich uses Neutral Networks and whose Accuracy is high\\nAlso Opinion vs Classifier is inpr\\nImportant\\nAchaa\\nGuys we need to complete our model by 15/16 March so that agar kidhar reject hua toh aur kidhar bhej sakte he\\nAtleast basic model ban jaye\\nRHS maâ€™am ne bola he ki last time jo humne diet ka app ka  bhara tha na government k liye wo shayad select hoga abhi\\nToh wo banana he\\nLet's see\\nGuys we will complete architectural diagrams tomorrow morning\\n@917506854640 please send the completed code for web scraping web.whatsapp.com which you have implemented\\nYash tell me which diagrams are to be done\\nSequence class architecure\\nI have sent the links\\nI have not got the link\\nAre toh tu banake bhejde na\\nWhat is the update Shubham\\nDekh message aur groups mill rahe hai\\nFormatting aur automation baaki hai\\nK, keep the code and data extracted in a presentable format\\nMake CSV file of data.\\nThat's formatting\\nWhich I will do tomorrow\\nYeah\\nSorry yaar time Nahi mil rha tha\\nPush the code on repository\\nWhich?\\nWe will also do the updatation required\\nScraping notebook\\nNo problem \\r\\n When is your internship getting over ?\\nBefore mse\\nThat's great\\nNo more internship work after that\\nData kis format me Chahiye bata\\nYeah\\n<Media omitted>\\nThis is the format.\\nCool\\nMP me complete Karna try karta hu\\nK\\nWhat's the update on this\\nBola nai\\nAsk\\nLeave it\\nShubham\\nAre maam saamne se bolegi jab result ayegi\\nSend the code of Web scraping\\nYash complete class diagram and sequence diagram\\nK\\nKal lab me kar dunga wo toh\\nKar diya\\n@917506854640  scrapping kiya ki nai?\\nSent\\nKal dikha denge toh\\nK\\nAlgo daal de bas\\nNahi\\nI will add flowchart\\nOkay\\n@917506854640 \\r\\n Scrap the exported chat of sender and receiver\\nDue to the increasing threat of coronavirus , it may be possible that the conference for which we are applying may get cancelled.\\r\\n So keep backup conferences ready before hand and pray that this virus is contained until next month.ðŸ™\\nWe might have to even change the conference\\nYeah keep backup conferences ready\\nHaan\\nSee the mail for abl\\n27 ko he\\nYeah\\nhttps://www.clarityinsights.com/blog/using-nlp-and-ai-to-detect-fake-news-with-99-accuracy\\nhttps://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/\\nhttps://nycdatascience.com/blog/student-works/identifying-fake-news-nlp/\\nrefer the above websites to get clear idea what we are going to do\\n@917506854640 What is the update for dataset?\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAeVyYUM3UO1",
        "colab_type": "code",
        "outputId": "0cbfdd92-8539-4b8c-e06b-e0dbc4ca6726",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "p = model_claim.predict(gen_test(s)[0])\n",
        "claim_classified = {}\n",
        "try :\n",
        "    for i in range(len(p)):\n",
        "        # print(d[i],end=\" \")\n",
        "        # print(type(metadata['labels'][np.where(p[i] == np.amax(p[i]))[0][0]][0]))\n",
        "        if(metadata['labels'][np.where(p[i] == np.amax(p[i]))[0][0]][0]==\"sd\" or metadata['labels'][np.where(p[i] == np.amax(p[i]))[0][0]][0]==\"sv\"):\n",
        "            claim_classified[d[i]] = metadata['labels'][np.where(p[i] == np.amax(p[i]))[0][0]][0]\n",
        "except Exception as e:\n",
        "        print(e)\n",
        "keys = claim_classified.keys()\n",
        "for i in keys:\n",
        "  try:\n",
        "    if(len(i)<70):\n",
        "      claim_classified[i] = None\n",
        "  except Exception as e:\n",
        "    print(end=\"\")\n",
        "  if(claim_classified[i]!=None):\n",
        "    print(i)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "list index out of range\n",
            "@917506854640 tu kal nai he toh aaj research kar konse topic pe karna he hume\n",
            "Genomics â€“ Genomics is the study of DNA of organisms. Machine Learning systems can help in finding the location of protein-encoding genes in a DNA structure. Gene prediction is performed by using two types of searches named as extrinsic and intrinsic. Machine Learning is used in problems related to DNA alignment.\n",
            "Jis conference me paper daalo ge udhar jo current topic chal raha he uspe karo\n",
            "This guy has got logic in th text generated that's exactly what we want\n",
            "RHS maâ€™am ne bola he ki last time jo humne diet ka app ka  bhara tha na government k liye wo shayad select hoga abhi\n",
            "Due to the increasing threat of coronavirus , it may be possible that the conference for which we are applying may get cancelled.\r\n",
            " So keep backup conferences ready before hand and pray that this virus is contained until next month.ðŸ™\n",
            "https://www.clarityinsights.com/blog/using-nlp-and-ai-to-detect-fake-news-with-99-accuracy\n",
            "https://nycdatascience.com/blog/student-works/identifying-fake-news-nlp/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yKs22-cQ_Ri",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.summarization import summarize\n",
        "from gensim.summarization import keywords"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCh2niCzS2LW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "claims = pd.read_excel(\"/content/drive/My Drive/Colab Notebooks/Corona.xlsx\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUKl4S1QV5FN",
        "colab_type": "code",
        "outputId": "3f027c04-036d-479b-8098-345b04208afc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        }
      },
      "source": [
        "# claim_df.drop(\"Summary\",axis=1,inplace=True)\n",
        "claims.replace(r'(\\n|\\r|\\t|\\s)',' ', regex=True,inplace=True)\n",
        "claims['Keywords'] = pd.Series(index=df.index)\n",
        "for i in range(len(claim_df)):\n",
        "  claims['Keywords'].iloc[i] = keywords(claims['Claims'].iloc[i]).split(\"\\n\")"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:205: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self._setitem_with_indexer(indexer, value)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oc_zhiDURFjZ",
        "colab_type": "code",
        "outputId": "0ce89cb8-030e-4a80-fdbe-239f5369c784",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "claims"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Claims</th>\n",
              "      <th>Label</th>\n",
              "      <th>Keywords</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Latest updates ... New Bollywood movie in prod...</td>\n",
              "      <td>FAKE</td>\n",
              "      <td>[security, hero, like, pyar, shah, rukh, bolly...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NOTE: Important msg from Kerala police to all ...</td>\n",
              "      <td>FAKE</td>\n",
              "      <td>[kerala, pls]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\"NASA satellite videos LIVE telecast have show...</td>\n",
              "      <td>FAKE</td>\n",
              "      <td>[satellite, waves, wave, covid]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Hello everyone   Dear friends tomorrow is one ...</td>\n",
              "      <td>FAKE</td>\n",
              "      <td>[pliz, viruses, friends, help, helpful, janta]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>79 suspected cases  of Covid 19 detected in ou...</td>\n",
              "      <td>FAKE</td>\n",
              "      <td>[dahisar, area, areas, stay, solution, quarant...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>☢️☣️Antidote for Wuhan Virus - coronavirus (an...</td>\n",
              "      <td>FAKE</td>\n",
              "      <td>[indian, rasam, virus, powder, way, spreading,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>AN OPINION GIVEN : 5 pm ; 22nd Mar, 'amavasya'...</td>\n",
              "      <td>FAKE</td>\n",
              "      <td>[virus, vibrations, vibration, amavasya darkes...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Russian president Vladimir Putin released 800 ...</td>\n",
              "      <td>FAKE</td>\n",
              "      <td>[president]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Unicef The corona virus is large in size with...</td>\n",
              "      <td>FAKE</td>\n",
              "      <td>[hot, corona virus, killing, killed, kills, st...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Central Railway  Press Release  *Trains cancel...</td>\n",
              "      <td>True</td>\n",
              "      <td>[express, mumbai, nagpur, pune, preventive, ce...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Most Urgent,Very Serious, Important informatio...</td>\n",
              "      <td>FAKE</td>\n",
              "      <td>[fever, information, prevention, drink, influe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Pass it please.  Good news, Wuhan's coronaviru...</td>\n",
              "      <td>FAKE</td>\n",
              "      <td>[boiled garlic, boil, water]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Corona virus [sic] before it reaches the lungs...</td>\n",
              "      <td>FAKE</td>\n",
              "      <td>[virus, water]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>blasting hot air from a blow dryer into your s...</td>\n",
              "      <td>FAKE</td>\n",
              "      <td>[hot, water]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>From a friend at the Stanford hospital board. ...</td>\n",
              "      <td>FAKE</td>\n",
              "      <td>[virus, self, basically, hospital, coronavirus...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Everyone should ensure your mouth &amp; throat are...</td>\n",
              "      <td>FAKE</td>\n",
              "      <td>[mouth]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>MHA statement: There have been cases where peo...</td>\n",
              "      <td>True</td>\n",
              "      <td>[including, enforcing]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>More than one billion people have been asked t...</td>\n",
              "      <td>True</td>\n",
              "      <td>[pandemic, afp]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Dr. Li Wenliang, China's hero doctor who was p...</td>\n",
              "      <td>FAKE</td>\n",
              "      <td>[tea, stimulant, stimulate compounds, casefile...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Maharashtra CM Uddhav Thackeray announces impo...</td>\n",
              "      <td>True</td>\n",
              "      <td>[coronavirus, ministry, hospital, hospitals, u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Lok Sabha members clap for people at forefront...</td>\n",
              "      <td>True</td>\n",
              "      <td>[coronavirus, govt, high, cases, challenge, ma...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>The Telangana government has announced restric...</td>\n",
              "      <td>True</td>\n",
              "      <td>[movement, essential, emergency]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Reliance steps up efforts to combat pandemic: ...</td>\n",
              "      <td>True</td>\n",
              "      <td>[reliance, free, production, led, service, face]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Nestle CEO tells staff to get ready for corona...</td>\n",
              "      <td>True</td>\n",
              "      <td>[schneider, extra, chocolate, products, instant]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>Migrant workers returning to Odisha may be at ...</td>\n",
              "      <td>True</td>\n",
              "      <td>[risk, risks, isolation, workers]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>British-Indian quiz master fears he may have C...</td>\n",
              "      <td>True</td>\n",
              "      <td>[media, quizzers, covid, paul, time, angry, re...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>Nepal confirms second coronavirus case Nepal ...</td>\n",
              "      <td>True</td>\n",
              "      <td>[confirms, confirmed, disease, bhanubhakta, dh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>First doctor dies from coronavirus in Pak A 26...</td>\n",
              "      <td>True</td>\n",
              "      <td>[patients, riaz, doctor, doctors, gilgit, cent...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>India's move to cancel foreign flights will le...</td>\n",
              "      <td>True</td>\n",
              "      <td>[international, foreign flights, carrying, early]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>TVS Motor shuts down plants for two days Two-w...</td>\n",
              "      <td>True</td>\n",
              "      <td>[tvs, motor, said, effective]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>Lockdown comes into force in Punjab, Chandigar...</td>\n",
              "      <td>True</td>\n",
              "      <td>[till, officials, chandigarh, shall, grocery, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>68-yr-old man who recovered from COVID-19 dies...</td>\n",
              "      <td>True</td>\n",
              "      <td>[reported, test report, tested, hospital, civi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>Japan PM Abe says postponing Olympics may be u...</td>\n",
              "      <td>True</td>\n",
              "      <td>[olympics, olympic, minister, shinzo]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>As carona virus life at one place is 12 hrs an...</td>\n",
              "      <td>FAKE</td>\n",
              "      <td>[virus]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>Uber suspends services in cities across India ...</td>\n",
              "      <td>True</td>\n",
              "      <td>[uber, services, travel]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>The Odisha government on Monday extended the s...</td>\n",
              "      <td>True</td>\n",
              "      <td>[lockdown, bus, transport, areas, extended, jh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>ICMR director general Balram Bhargava on Monda...</td>\n",
              "      <td>True</td>\n",
              "      <td>[said, approved, approval, icmr, confirmed, pa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>Compelled' to impose curfew, seal district bor...</td>\n",
              "      <td>True</td>\n",
              "      <td>[district, districts, borders, curfew, maharas...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>Domestic commercial flights suspended from 24 ...</td>\n",
              "      <td>True</td>\n",
              "      <td>[commercial flights]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>Manipur has reported its first case of coronav...</td>\n",
              "      <td>True</td>\n",
              "      <td>[lockdown, territories covering, positive, kin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>Hurt by the novel coronavirus pandemic, the au...</td>\n",
              "      <td>True</td>\n",
              "      <td>[temporarily, coronavirus, lockdown]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>Tamil Nadu Chief Minister announces Rs 1,000 t...</td>\n",
              "      <td>True</td>\n",
              "      <td>[workers, nadu, drivers, auto, special]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>65-year-old man dies at Mumbai’s Kasturba Hosp...</td>\n",
              "      <td>True</td>\n",
              "      <td>[death, maharashtra]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>Many people are still not taking the lockdown ...</td>\n",
              "      <td>True</td>\n",
              "      <td>[follow, followed, seriously]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>Tamil Nadu will shut its border with Karnataka...</td>\n",
              "      <td>True</td>\n",
              "      <td>[like, states, state, covid, saturday, tamil, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>PM has called for Janta curfew,one day's coope...</td>\n",
              "      <td>True</td>\n",
              "      <td>[coronavirus, ministry, lav, saidhealth, affec...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>UP govt issues orders on not stopping vehicles...</td>\n",
              "      <td>True</td>\n",
              "      <td>[vehicles carrying, issues, issued, commission...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>12 COVID-19 patients cured in Maharashtra As m...</td>\n",
              "      <td>True</td>\n",
              "      <td>[patients, civic official said, municipal, swab]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>India has tremendous capacity in eradicating c...</td>\n",
              "      <td>True</td>\n",
              "      <td>[india, coronavirus, pandemic, intervention, n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>The iconic Jama Masjid will remain closed for ...</td>\n",
              "      <td>True</td>\n",
              "      <td>[till, remain, jama, outbreak]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>As the novel coronavirus or COVID-19 cases ros...</td>\n",
              "      <td>True</td>\n",
              "      <td>[lockdown, minister, authorities, returns, inf...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               Claims  ...                                           Keywords\n",
              "0   Latest updates ... New Bollywood movie in prod...  ...  [security, hero, like, pyar, shah, rukh, bolly...\n",
              "1   NOTE: Important msg from Kerala police to all ...  ...                                      [kerala, pls]\n",
              "2   \"NASA satellite videos LIVE telecast have show...  ...                    [satellite, waves, wave, covid]\n",
              "3   Hello everyone   Dear friends tomorrow is one ...  ...     [pliz, viruses, friends, help, helpful, janta]\n",
              "4   79 suspected cases  of Covid 19 detected in ou...  ...  [dahisar, area, areas, stay, solution, quarant...\n",
              "5   ☢️☣️Antidote for Wuhan Virus - coronavirus (an...  ...  [indian, rasam, virus, powder, way, spreading,...\n",
              "6   AN OPINION GIVEN : 5 pm ; 22nd Mar, 'amavasya'...  ...  [virus, vibrations, vibration, amavasya darkes...\n",
              "7   Russian president Vladimir Putin released 800 ...  ...                                        [president]\n",
              "8    Unicef The corona virus is large in size with...  ...  [hot, corona virus, killing, killed, kills, st...\n",
              "9   Central Railway  Press Release  *Trains cancel...  ...  [express, mumbai, nagpur, pune, preventive, ce...\n",
              "10  Most Urgent,Very Serious, Important informatio...  ...  [fever, information, prevention, drink, influe...\n",
              "11  Pass it please.  Good news, Wuhan's coronaviru...  ...                       [boiled garlic, boil, water]\n",
              "12  Corona virus [sic] before it reaches the lungs...  ...                                     [virus, water]\n",
              "13  blasting hot air from a blow dryer into your s...  ...                                       [hot, water]\n",
              "14  From a friend at the Stanford hospital board. ...  ...  [virus, self, basically, hospital, coronavirus...\n",
              "15  Everyone should ensure your mouth & throat are...  ...                                            [mouth]\n",
              "16  MHA statement: There have been cases where peo...  ...                             [including, enforcing]\n",
              "17  More than one billion people have been asked t...  ...                                    [pandemic, afp]\n",
              "18  Dr. Li Wenliang, China's hero doctor who was p...  ...  [tea, stimulant, stimulate compounds, casefile...\n",
              "19  Maharashtra CM Uddhav Thackeray announces impo...  ...  [coronavirus, ministry, hospital, hospitals, u...\n",
              "20  Lok Sabha members clap for people at forefront...  ...  [coronavirus, govt, high, cases, challenge, ma...\n",
              "21  The Telangana government has announced restric...  ...                   [movement, essential, emergency]\n",
              "22  Reliance steps up efforts to combat pandemic: ...  ...   [reliance, free, production, led, service, face]\n",
              "23  Nestle CEO tells staff to get ready for corona...  ...   [schneider, extra, chocolate, products, instant]\n",
              "24  Migrant workers returning to Odisha may be at ...  ...                  [risk, risks, isolation, workers]\n",
              "25  British-Indian quiz master fears he may have C...  ...  [media, quizzers, covid, paul, time, angry, re...\n",
              "26   Nepal confirms second coronavirus case Nepal ...  ...  [confirms, confirmed, disease, bhanubhakta, dh...\n",
              "27  First doctor dies from coronavirus in Pak A 26...  ...  [patients, riaz, doctor, doctors, gilgit, cent...\n",
              "28  India's move to cancel foreign flights will le...  ...  [international, foreign flights, carrying, early]\n",
              "29  TVS Motor shuts down plants for two days Two-w...  ...                      [tvs, motor, said, effective]\n",
              "30  Lockdown comes into force in Punjab, Chandigar...  ...  [till, officials, chandigarh, shall, grocery, ...\n",
              "31  68-yr-old man who recovered from COVID-19 dies...  ...  [reported, test report, tested, hospital, civi...\n",
              "32  Japan PM Abe says postponing Olympics may be u...  ...              [olympics, olympic, minister, shinzo]\n",
              "33  As carona virus life at one place is 12 hrs an...  ...                                            [virus]\n",
              "34  Uber suspends services in cities across India ...  ...                           [uber, services, travel]\n",
              "35  The Odisha government on Monday extended the s...  ...  [lockdown, bus, transport, areas, extended, jh...\n",
              "36  ICMR director general Balram Bhargava on Monda...  ...  [said, approved, approval, icmr, confirmed, pa...\n",
              "37  Compelled' to impose curfew, seal district bor...  ...  [district, districts, borders, curfew, maharas...\n",
              "38  Domestic commercial flights suspended from 24 ...  ...                               [commercial flights]\n",
              "39  Manipur has reported its first case of coronav...  ...  [lockdown, territories covering, positive, kin...\n",
              "40  Hurt by the novel coronavirus pandemic, the au...  ...               [temporarily, coronavirus, lockdown]\n",
              "41  Tamil Nadu Chief Minister announces Rs 1,000 t...  ...            [workers, nadu, drivers, auto, special]\n",
              "42  65-year-old man dies at Mumbai’s Kasturba Hosp...  ...                               [death, maharashtra]\n",
              "43  Many people are still not taking the lockdown ...  ...                      [follow, followed, seriously]\n",
              "44  Tamil Nadu will shut its border with Karnataka...  ...  [like, states, state, covid, saturday, tamil, ...\n",
              "45  PM has called for Janta curfew,one day's coope...  ...  [coronavirus, ministry, lav, saidhealth, affec...\n",
              "46  UP govt issues orders on not stopping vehicles...  ...  [vehicles carrying, issues, issued, commission...\n",
              "47  12 COVID-19 patients cured in Maharashtra As m...  ...   [patients, civic official said, municipal, swab]\n",
              "48  India has tremendous capacity in eradicating c...  ...  [india, coronavirus, pandemic, intervention, n...\n",
              "49  The iconic Jama Masjid will remain closed for ...  ...                     [till, remain, jama, outbreak]\n",
              "50  As the novel coronavirus or COVID-19 cases ros...  ...  [lockdown, minister, authorities, returns, inf...\n",
              "\n",
              "[51 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOBB89kMiqlF",
        "colab_type": "code",
        "outputId": "f2647f4b-71c0-4cad-c870-02ed73b09ad8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# sentences = list(result_df.Article)\n",
        "# sentences = summaries\n",
        "sentences = ['Hi']\n",
        "# Each sentence is encoded as a 1-D vector with 78 columns\n",
        "sentence_embeddings = model.encode(sentences)\n",
        "print(type(sentence_embeddings))\n",
        "\n",
        "# print('Sample BERT embedding vector - length', len(sentence_embeddings[0]))\n",
        "\n",
        "# print('Sample BERT embedding vector - note includes negative values', sentence_embeddings[0])\n",
        "# print(doc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoAtGFlFeRC9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def article_emdeddings(result_df):     \n",
        "    embeddings = model.encode(list(result_df.Summary))\n",
        "    result_df['Embedding'] = embeddings\n",
        "    return result_df\n",
        "\n",
        "def claim_embeddings(claim_df,i):\n",
        "    embeddings = model.encode([claim_df.Claims.iloc[i]])\n",
        "    claim_df[\"Embedding\"].iloc[i] = embeddings\n",
        "    # print(claim_df)\n",
        "    return claim_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzAZ0dvjRPhd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_distances,manhattan_distances,paired_cosine_distances\n",
        "from sklearn.metrics.pairwise import euclidean_distances,cosine_similarity\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atGjBianhiS4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def output(claim_df,result_df,k):\n",
        "  queries = [claim_df.Claims.iloc[k]]\n",
        "  query_embeddings = claim_df.Embedding.iloc[k] \n",
        "  sentence_embeddings = list(result_df['Embedding'])\n",
        "  # print(queries)\n",
        "  # print(query_embeddings)\n",
        "  # print(sentence_embeddings.shape)\n",
        "  number_top_matches = 2\n",
        "  final=[]\n",
        "  for query, query_embedding in zip(queries, query_embeddings):\n",
        "      # distances = scipy.spatial.distance.cdist([query_embedding], sentence_embeddings, \"cosine\")[0]\n",
        "      distances = cosine_distances([query_embedding],sentence_embeddings)[0]\n",
        "      results = zip(range(len(distances)), distances)\n",
        "      results = sorted(results, key=lambda x: x[1])\n",
        "\n",
        "      print(\"\\n\\n======================\\n\\n\")\n",
        "      # print(\"Query:\", query)\n",
        "      # print(\"\\nTop 3 most similar sentences in corpus:\")\n",
        "      sources=[]\n",
        "      for idx, distance in results[0:number_top_matches]:\n",
        "          # final[result_df.Article.iloc[idx]] = float(1-distance)\n",
        "          # print(result_df.Article.iloc[idx].strip(), \"\\n(Cosine Score: %.4f)\" % (1-distance))\n",
        "          # print(\"\\n\",result_df.News_Url.iloc[idx])\n",
        "          sources.append({\"cosine_distance\":float(distance),\"cosine_similarity\":float(1-distance),\"url\":result_df.News_Url.iloc[idx]})\n",
        "      tot=0\n",
        "      for s in sources:\n",
        "        tot+=s['cosine_similarity']\n",
        "      mean=tot/number_top_matches\n",
        "      if(round(mean*100,2)>40):\n",
        "        final.append({\"Value\":True,\"sources\":sources,\"claim\":k})\n",
        "      else:\n",
        "        final.append({\"Value\":False,\"sources\":sources,\"claim\":k})\n",
        "  return final\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haa0_CY5Ykm9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "claims['Embedding'] = None\n",
        "# claim_df\n",
        "# claim_df\n",
        "claim_df = claims"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40ae9nDsg0Tc",
        "colab_type": "code",
        "outputId": "105b3de5-ba97-4d27-90d8-18264668da53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 990
        }
      },
      "source": [
        "a = []\n",
        "count = []\n",
        "final = []\n",
        "k1 = [6,14,38,41]\n",
        "for k in range(len(claim_df)):\n",
        "  key = claims.Keywords.iloc[k]\n",
        "  for i in range(len(df)):\n",
        "    count = 0\n",
        "    for j in key:\n",
        "      count += df.Keywords.iloc[i].count(j)\n",
        "    if(count>=3):\n",
        "      a.append(i)\n",
        "  if(len(a)==0):\n",
        "    print(\"Claim at : {0} is FAKE. \".format(k))\n",
        "    continue\n",
        "  result_df = pd.DataFrame(columns=df.columns)\n",
        "  print(a) \n",
        "  for i in set(a):\n",
        "    result_df = result_df.append(df.iloc[i])\n",
        "  result_df = article_emdeddings(result_df)\n",
        "  claim_df = claim_embeddings(claims,k)\n",
        "  final.append(output(claim_df,result_df,k))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Claim at : 0 is FAKE. \n",
            "Claim at : 1 is FAKE. \n",
            "[2424, 2544, 3002]\n",
            "\n",
            "\n",
            "======================\n",
            "\n",
            "\n",
            "[2424, 2544, 3002]\n",
            "\n",
            "\n",
            "======================\n",
            "\n",
            "\n",
            "[2424, 2544, 3002, 417]\n",
            "\n",
            "\n",
            "======================\n",
            "\n",
            "\n",
            "[2424, 2544, 3002, 417, 41, 154, 157, 167, 213, 240, 369, 371, 387, 453, 457, 661, 768, 880, 949, 960, 977, 1036, 1088, 1168, 1206, 1207, 1496, 1499, 1514, 1637, 1639, 1649, 1724, 1844, 1845, 2041, 2055, 2085, 2126, 2132, 2293, 2421, 2614, 2618, 2719, 2782, 2897, 2900, 2910, 2945, 3103, 3134, 3143, 3149, 3204]\n",
            "\n",
            "\n",
            "======================\n",
            "\n",
            "\n",
            "[2424, 2544, 3002, 417, 41, 154, 157, 167, 213, 240, 369, 371, 387, 453, 457, 661, 768, 880, 949, 960, 977, 1036, 1088, 1168, 1206, 1207, 1496, 1499, 1514, 1637, 1639, 1649, 1724, 1844, 1845, 2041, 2055, 2085, 2126, 2132, 2293, 2421, 2614, 2618, 2719, 2782, 2897, 2900, 2910, 2945, 3103, 3134, 3143, 3149, 3204, 2662, 2782, 3103]\n",
            "\n",
            "\n",
            "======================\n",
            "\n",
            "\n",
            "[2424, 2544, 3002, 417, 41, 154, 157, 167, 213, 240, 369, 371, 387, 453, 457, 661, 768, 880, 949, 960, 977, 1036, 1088, 1168, 1206, 1207, 1496, 1499, 1514, 1637, 1639, 1649, 1724, 1844, 1845, 2041, 2055, 2085, 2126, 2132, 2293, 2421, 2614, 2618, 2719, 2782, 2897, 2900, 2910, 2945, 3103, 3134, 3143, 3149, 3204, 2662, 2782, 3103]\n",
            "\n",
            "\n",
            "======================\n",
            "\n",
            "\n",
            "[2424, 2544, 3002, 417, 41, 154, 157, 167, 213, 240, 369, 371, 387, 453, 457, 661, 768, 880, 949, 960, 977, 1036, 1088, 1168, 1206, 1207, 1496, 1499, 1514, 1637, 1639, 1649, 1724, 1844, 1845, 2041, 2055, 2085, 2126, 2132, 2293, 2421, 2614, 2618, 2719, 2782, 2897, 2900, 2910, 2945, 3103, 3134, 3143, 3149, 3204, 2662, 2782, 3103, 111, 496, 719, 1187, 1716, 1863, 2847]\n",
            "\n",
            "\n",
            "======================\n",
            "\n",
            "\n",
            "[2424, 2544, 3002, 417, 41, 154, 157, 167, 213, 240, 369, 371, 387, 453, 457, 661, 768, 880, 949, 960, 977, 1036, 1088, 1168, 1206, 1207, 1496, 1499, 1514, 1637, 1639, 1649, 1724, 1844, 1845, 2041, 2055, 2085, 2126, 2132, 2293, 2421, 2614, 2618, 2719, 2782, 2897, 2900, 2910, 2945, 3103, 3134, 3143, 3149, 3204, 2662, 2782, 3103, 111, 496, 719, 1187, 1716, 1863, 2847, 360, 369, 951, 1075, 1088, 1315, 1496, 1639, 1844, 1846, 2085, 2144, 2153, 2520, 2614]\n",
            "\n",
            "\n",
            "======================\n",
            "\n",
            "\n",
            "[2424, 2544, 3002, 417, 41, 154, 157, 167, 213, 240, 369, 371, 387, 453, 457, 661, 768, 880, 949, 960, 977, 1036, 1088, 1168, 1206, 1207, 1496, 1499, 1514, 1637, 1639, 1649, 1724, 1844, 1845, 2041, 2055, 2085, 2126, 2132, 2293, 2421, 2614, 2618, 2719, 2782, 2897, 2900, 2910, 2945, 3103, 3134, 3143, 3149, 3204, 2662, 2782, 3103, 111, 496, 719, 1187, 1716, 1863, 2847, 360, 369, 951, 1075, 1088, 1315, 1496, 1639, 1844, 1846, 2085, 2144, 2153, 2520, 2614]\n",
            "\n",
            "\n",
            "======================\n",
            "\n",
            "\n",
            "[2424, 2544, 3002, 417, 41, 154, 157, 167, 213, 240, 369, 371, 387, 453, 457, 661, 768, 880, 949, 960, 977, 1036, 1088, 1168, 1206, 1207, 1496, 1499, 1514, 1637, 1639, 1649, 1724, 1844, 1845, 2041, 2055, 2085, 2126, 2132, 2293, 2421, 2614, 2618, 2719, 2782, 2897, 2900, 2910, 2945, 3103, 3134, 3143, 3149, 3204, 2662, 2782, 3103, 111, 496, 719, 1187, 1716, 1863, 2847, 360, 369, 951, 1075, 1088, 1315, 1496, 1639, 1844, 1846, 2085, 2144, 2153, 2520, 2614, 1863]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7Z-nVc-cJab",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JU6lg0JAdO0B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "correct = 0\n",
        "total = len(final)\n",
        "for f in final:\n",
        "  for f1 in f:\n",
        "    print(f1['Value'])\n",
        "    if(f1['Value']==True and claim_df.Label.iloc[f1['claim']]==True):\n",
        "      correct+=1\n",
        "    elif(f1['Value']==False and claim_df.Label.iloc[f1['claim']]=='FAKE'):\n",
        "      correct+=1\n",
        "\n",
        "print(\"Accuracy : \",round(float(correct/total)*100,2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVl-Ui1yeDjs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "0c82fc83-01ec-437f-9e8a-f3af309a8000"
      },
      "source": [
        "s = [claim_df.Label.iloc[k] for k in k1]\n",
        "print(s)"
      ],
      "execution_count": 308,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[True, True, True, True]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEOA9fC1RXz5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Sematic Search Form\n",
        "\n",
        "# code adapted from https://github.com/UKPLab/sentence-transformers/blob/master/examples/application_semantic_search.py\n",
        "\n",
        "query = 'Nobody has sane thoughts' #@param {type: 'string'}\n",
        "# doc = \"Shah Rukh Khan is a college student, whose girlfriend is touring China. She gets infected with coronavirus & has been quarantined in a high security hospital in China. When the hero hears this, he consults a local baba, who gives a special medicine, which is nothing  but _Hindustan ki mitti_ - soil of India. The hero now faces several hurdles while trying to deliver this medicine to his sweetheart, like breaching the border as there is travel restriction, extreme security at the hospital & a villain trying to sabotage the mission by replacing the medicine with 'Pakistan ki mitti'. How the hero accomplishes his mission is rest of the story.\\\n",
        "# The title of the movie is: ...\\\n",
        "# CORONA PYAR HAI\"\n",
        "# print(doc)\n",
        "queries = [doc]\n",
        "query_embeddings = model.encode(queries)\n",
        "\n",
        "# Find the closest 3 sentences of the corpus for each query sentence based on cosine similarity\n",
        "number_top_matches =  3#@param {type: \"number\"}\n",
        "\n",
        "print(\"Semantic Search Results\")\n",
        "\n",
        "\n",
        "for query, query_embedding in zip(queries, query_embeddings):\n",
        "    # distances = scipy.spatial.distance.cdist([query_embedding], sentence_embeddings, \"cosine\")[0]\n",
        "    distances = cosine_distances([query_embedding], sentence_embeddings)[0]\n",
        "    results = zip(range(len(distances)), distances)\n",
        "    results = sorted(results, key=lambda x: x[1])\n",
        "\n",
        "    print(\"\\n\\n======================\\n\\n\")\n",
        "    # print(\"Query:\", query)\n",
        "    print(\"\\nTop 3 most similar sentences in corpus:\")\n",
        "\n",
        "    final = {}\n",
        "    for idx, distance in results[0:number_top_matches]:\n",
        "        final[sentences[idx]] = float(1-distance)\n",
        "        print(sentences[idx].strip(), \"\\n(Cosine Score: %.4f)\" % (1-distance))\n",
        "        print(result_df.News_Url.iloc[idx])\n",
        "scores = list(final.values())\n",
        "if(round(np.mean(scores)*100,2)>70):\n",
        "  print(\"The Article is True\")\n",
        "else:\n",
        "  print(\"The Article is False\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmwOF1ysRbh7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# article_embeddings = model.encode(list(result_df.Article))\n",
        "# summary_embiddings = model.encode(list(result_df.Summary))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}